<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	id="analyzers" xmlns="http://www.oasis-open.org/docbook/4.5" xmlns:xl="http://www.w3.org/1999/xlink"
	xsi:schemaLocation="http://www.oasis-open.org/docbook/4.5 http://www.oasis-open.org/docbook/xsd/4.5/docbook.xsd">

	<title>Analyze</title>

	<chapterinfo>
		<abstract>
			<para>This chapter deals with one of the most important concepts in
				DataCleaner: Analysis of data quality.
			</para>
			<para>An analyzer is a component that consumes a (set of) column(s)
				and generates an
				analysis result based on the values in the consumed
				columns.
			</para>
			<para>Here is an example of a configuration panel pertaining to an
				analyzer:
			</para>
			<mediaobject>
				<imageobject>
					<imagedata fileref="analysis_job_04_analyzer.jpg" format="JPG"
						scalefit="1" />
				</imageobject>
			</mediaobject>
			<para>In the panel there will always be one or more selections of
				columns. The configuration panel may also contain additional
				properties for configuration.
			</para>
		</abstract>
	</chapterinfo>

	<section id="boolean_analyzer">
		<title>Boolean analyzer</title>
		<para>Boolean analyzer is an analyzer targeted at boolean values. For
			a single boolean column it is quite simple: It will show the
			distribution of true/false (and optionally null) values in a column.
			For several columns it will also show the value combinations and the
			frequencies of the combinations. The combination matrix makes the
			Boolean analyzer a handy analyzer for use with combinations of
			matching transformers and other transformers that yield boolean
			values.
		</para>
		<para>Boolean analyzer has no configuration parameters, except for the
			input columns.
		</para>
	</section>

	<section id="completeness_analyzer">
		<title>Completeness analyzer</title>
		<para>The completeness analyzer provides a really simple way to check
			that all required fields in your records have been filled. Think of
			it like a big "not null" check across multiple fields. In combination
			with the monitoring application, this analyzer makes it easy to track
			which records needs additional information.
		</para>
		<para>Here is a screenshot of the configuration panel of the
			Completeness analyzer:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_completeness_analyzer.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>The configuration properties of the Completeness analyzer are:
		</para>
		<table>
			<title>Completeness analyzer properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Values</entry>
						<entry>Select the columns you want to evaluate with your
							completeness analyzer. For each selected column you get to choose
							whether the analyzer should simply do a null-check, or if it
							should also check for blank values.
						</entry>
					</row>
					<row>
						<entry>Evaluation mode</entry>
						<entry>
							This determines the mode that the completeness check runs in.
							Here you can configure whether the analyzer should consider
							records as "incomplete" if
							<emphasis>any</emphasis>
							of the selected values are null/blank, or if
							<emphasis>all</emphasis>
							the values need to be null/blank before the record is counted as
							incomplete.
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="character_set_distribution">
		<title>Character set distribution</title>
		<para>The Character set distribution analyzer inspects and maps text
			characters according to character set affinity, such as Latin,
			Hebrew, Cyrillic, Chinese and more.
		</para>
		<para>Such analysis is convenient for getting insight into the
			international aspects of your data. Are you able to read and
			understand all your data? Will it work in your non-internationalized
			systems?
		</para>
	</section>

	<section id="date_gap_analyzer">
		<title>Date gap analyzer</title>
		<para>The Date gap analyzer is used to identify gaps in recorded time
			series. This analyzer is useful for example if you have employee time
			registration systems which record FROM and TO dates. It will allow
			you to identify if there are unexpected gaps in the data.
		</para>
	</section>

	<section id="date_time_analyzer">
		<title>Date/time analyzer</title>
		<para>The Date/time analyzer provides general purpose profiling
			metrics for temporal column types such as DATE, TIME and TIMESTAMP
			columns.
		</para>
	</section>

	<section id="number_analyzer">
		<title>Number analyzer</title>
		<para>The number analyzer provides general purpose profiling
			metrics
			for numerical column types.
		</para>
	</section>

	<section id="pattern_finder">
		<title>Pattern finder</title>
		<para>The pattern finder is one of the more advanced, but also very
			popular analyzers of DataCleaner.
		</para>
		<para>Here is a screenshot of the configuration panel of the Pattern
			finder:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_pattern_finder.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>From the screenshot we can see that the Pattern finder has these
			configuration properties:
		</para>
		<table>
			<title>Pattern finder properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Group column</entry>
						<entry>Allows you to define a pattern group column. With a pattern
							group column you can separate the identified patterns into
							separate buckets/groups. Imagine for example that you want to
							check if the phone numbers of your customers are consistent. If
							you have an international customer based, you should then group
							by a country column to make sure that phone patterns identified
							are not matched with phone patterns from different countries.
						</entry>
					</row>
					<row>
						<entry>Discriminate text case</entry>
						<entry>Defines whether or not to discriminate (ie. consider as
							different pattern parts) based on text case. If true
							"DataCleaner" and "datacleaner" will be considered instances of
							different patterns, if false they will be matched within same
							pattern.
						</entry>
					</row>
					<row>
						<entry>Discriminate negative numbers</entry>
						<entry>When parsing numbers, this property defines if negative
							numbers should be discriminated from positive numbers.
						</entry>
					</row>
					<row>
						<entry>Discriminate decimals</entry>
						<entry>When parsing numbers, this property defines if decimal
							numbers should be discriminated from integers.
						</entry>
					</row>
					<row>
						<entry>Enable mixed tokens</entry>
						<entry>
							<para>Defines whether or not to categorize tokens that contain
								both letters and digits as "mixed", or alternatively as two
								separate tokens. Mixed tokens are represented using questionmark
								('?')
								symbols.
							</para>
							<para>This is one of the more important configuration properties.
								For example if mixed tokens are enabled (default), all these
								values will
								be matched against the same pattern: foo123, 123foo,
								foobar123, foo123bar. If mixed tokens are NOT enabled only
								foo123 and foobar123 will be matched (because 123foo and
								foo123bar represent different combinations of letter and digit
								tokens).
							</para>
						</entry>
					</row>
					<row>
						<entry>Ignore repeated spaces</entry>
						<entry>Defines whether or not to discriminate based on amount of
							whitespaces.
						</entry>
					</row>
					<row>
						<entry>Upper case patterns expand in size</entry>
						<entry>Defines whether or not upper case tokens automatically
							"expand" in size. Expandability refers to whether or not the
							found patterns will include matches if a candidate has the same
							type of token, but with a different size. The default
							configuration for upper case characters is false (ie. ABC is
							not
							matched with ABCD).
						</entry>
					</row>
					<row>
						<entry>Lower case patterns expand in size</entry>
						<entry>
							<para>Defines whether or not lower case tokens automatically
								"expand" in size. As with upper case expandability, this
								property
								refers to whether or not the found patterns will include
								matches
								if a candidate has the same type of token, but with a
								different
								size. The default configuration for lower case
								characters is
								true
								(ie. 'abc' is not matched with 'abc').
							</para>
							<para>The defaults in the two "expandability" configuration
								properties mean that eg. name pattern recognition is meaningful:
								'James' and 'John' both pertain to the same pattern ('Aaaaa'),
								while 'McDonald' pertain to a different pattern ('AaAaaaaa').
							</para>
						</entry>
					</row>
					<row>
						<entry>Predefined token name</entry>
						<entry>Predefined tokens make it possible to define a token to
							look for and classify using either just a fixed list of values or
							regular expressions. Typically this is used if the values contain
							some additional parts which you want to manually define a
							matching category for. The 'Predefined token name' property
							defines the name of such a category.
						</entry>
					</row>
					<row>
						<entry>Predefined token regexes</entry>
						<entry>Defines a number of string values and/or regular
							expressions which are used to match values against the
							(pre)defined token category.
						</entry>
					</row>
					<row>
						<entry>Decimal separator</entry>
						<entry>The decimal separator character, used when parsing numbers
						</entry>
					</row>
					<row>
						<entry>Thousand separator</entry>
						<entry>The thousand separator character, used when parsing numbers
						</entry>
					</row>
					<row>
						<entry>Minus sign</entry>
						<entry>The minus sign character, used when parsing numbers</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="reference_data_matcher_analyzer">
		<title>Reference data matcher</title>
		<para>The 'Reference data matcher' analyzer provides an easy means to
			match several
			columns against several dictionaries and/or several
			string patterns.
			The result is a matrix of match information for all
			columns and all
			matched resources.
		</para>
	</section>

	<section id="referential_integrity_analyzer">
		<title>Referential integrity</title>
		<para>With the 'Referential integrity' analyzer you can check that key
			relationships between records are intact. The analyzer will work with
			relationships within a single table, between tables and even between
			tables of different datastores.
		</para>
		<para>Here is a screenshot of the configuration panel of the
			Referential integrity analyzer:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_referential_integrity.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>
			Apply the analyzer on the table with the foreign key in the
			relationship, and configure it to do a check on the table that holds
			all the valid keys.
		</para>
		<table>
			<title>Referential integrity properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Cache lookups</entry>
						<entry>Whether or not the analyzer should speed up referential
							integrity checking by caching previous lookup results. Whether or
							not this will gain performance ultimately depends on the amount
							of repetition in the keys to be checked. If all foreign key
							values are more or less unique, it should definately be turned
							off. But if there is a fair amount of duplication in the foreign
							keys (e.g. orderlines referring to the same products or
							customers), then it makes the lookups faster.
						</entry>
					</row>
					<row>
						<entry>Ignore null values</entry>
						<entry>Defines whether or not "null" values should be ignored or
							if they should be considered as an integrity issue. When ignored,
							all records with null foreign key values will simply be discarded
							by the analyzer.
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="string_analyzer">
		<title>String analyzer</title>
		<para>The string analyzer provides general purpose profiling
			metrics
			for string column types. Of special concern to the string analyzer is
			the amount of words, characters, special signs, diacritics and other
			metrics that are virtal to understanding what kind of string values
			occur in the data.
		</para>
	</section>

	<section id="unique_key_check">
		<title>Unique key check</title>
		<para>The 'Unique key check' analyzer provides an easy way to verify
			that keys/IDs are unique - as it is usually expected.
		</para>
		<para>The properties of 'Unique key check' are:</para>
		<table>
			<title>Unique key check properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Column</entry>
						<entry>Pick the column that this analyzer should perform the
							uniqueness check on.
						</entry>
					</row>
					<row>
						<entry>Buffer size</entry>
						<entry>The buffer represents the internal resource for sorting and
							comparison of keys. Having a large buffer makes the analyzer run
							faster and take up fewer resources on disk, but at the expense of
							using memory. If your job is not already memory intensive, we
							recommend increasing the buffer size up to 1M.
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="value_distribution">
		<title>Value distribution</title>
		<para>The value distribution (often also referred to as 'Frequency
			analysis') allows you to identify all the values of a particular
			column. Furthermore you can investigate which rows pertain to
			specific values.
		</para>
		<para>Here are the configuration properties for the value distribution
			analyzer:
		</para>
		<table>
			<title>Value distribution properties</title>
			<tgroup cols="2">
				<thead>
					<row>
						<entry>Property</entry>
						<entry>Description</entry>
					</row>
				</thead>
				<tbody>
					<row>
						<entry>Group column</entry>
						<entry>Allows you to define a column for grouping the result. With
							a
							group column you can separate the identified value distributions
							into
							separate buckets/groups. Imagine for example that you want to
							check if the postal codes and city names correspond or if you
							just want to segment your value distribution on eg. country or
							gender or ...
						</entry>
					</row>
					<row>
						<entry>Record unique values</entry>
						<entry>By default all unique values will be included in the result
							of the value distribution. This can potentially cause memory
							issues if your analyzed columns contains a LOT of unique values
							(eg. if it's a unique key). If the actual unique values are not
							of interest, then uncheck this checkbox to only count (but not
							save for inspection) the unique values.
						</entry>
					</row>
					<row>
						<entry>Top n most frequent vales</entry>
						<entry>An optional number used if the analysis should only display
							eg. the "top 5 most frequent values". The result of the analysis
							will only contain top/bottom n most frequent values, if this
							property is supplied.
						</entry>
					</row>
					<row>
						<entry>Bottom n most frequent values</entry>
						<entry>An optional number used if the analysis should only display
							eg. the "bottom 5 most frequent values". The result of the
							analysis
							will only contain top/bottom n most frequent values, if
							this
							property is supplied.
						</entry>
					</row>
				</tbody>
			</tgroup>
		</table>
	</section>

	<section id="value_matcher">
		<title>Value matcher</title>
		<para>
			The value matcher works very similar to the
			<link linkend="value_distribution">Value distribution</link>
			, except for the fact that it takes a list of expected values and
			everything else is put into a group of 'unexpected values'. This
			division of values means a couple of things:
		</para>
		<orderedlist>
			<listitem>
				<para>You get a built-in validation mechanism. You expect maybe only
					'M' and 'F' values for your 'gender' column, and everything else is
					in a sense invalid, since it is unexpected.
				</para>
			</listitem>
			<listitem>
				<para>The division makes it easier to monitor specific values in the
					data quality monitoring web application.
				</para>
			</listitem>
			<listitem>
				<para>This analyzer scales much better for large datasets, since the
					groupings are deterministic and thus can be prepared for in the
					batch run.
				</para>
			</listitem>
		</orderedlist>
	</section>

	<section id="weekday_distribution">
		<title>Weekday distribution</title>
		<para>The weekday distribution provides a frequency analysis for date
			columns, where you can easily identify which weekdays a date field
			represents.
		</para>
	</section>

</chapter>
