<?xml version="1.0" encoding="UTF-8"?>
<chapter id="hadoop_interface" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.oasis-open.org/docbook/4.5"
	xsi:schemaLocation="http://www.oasis-open.org/docbook/4.5 http://www.oasis-open.org/docbook/xsd/4.5/docbook.xsd">

	<title>Apache Hadoop and Spark interface</title>

	<chapterinfo>
		<abstract>
			<para>
				DataCleaner allows big data processing on the Apache Hadoop platform.
				In this chapter we will guide you through the process of setting up
				and running your first DataCleaner job on Hadoop.
			</para>
		</abstract>
	</chapterinfo>


	<section id="hadoop_deployment">
		<title>Hadoop deployment overview</title>
		<para>Apache Hadoop is a distributed system with a number of key components of which a few are important to understand:</para>
		<orderedlist>
			<listitem>
				<para>
					<emphasis>YARN</emphasis>, which is often referred to as the 'operating system' of Hadoop. YARN is the managing entity which assigns resources to running a specific job or task.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>HDFS</emphasis>, which is the Hadoop Distributed File System. This is the location where data is located, but also the place where executables are often shared so that a distributed process can be picked up on many nodes in the cluster.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Namenode</emphasis>, is a dedicated node in the cluster which deals with HDFS and distribution of data to other nodes, so-called datanodes.
				</para>
			</listitem>
		</orderedlist>
		<para>In addition, the DataCleaner Hadoop support is built using Apache Spark, which is a data processing framework that works with Hadoop as well as other clustering technologies. A few important concepts of Apache Spark are useful to understand for DataCleaner's deployment on Hadoop:</para>
		<orderedlist>
			<listitem>
				<para>
					<emphasis>Cluster manager</emphasis>, which is the component that negotiates with the cluster - for instance Hadoop/YARN. From the perspective of Apache Spark, YARN is a <emphasis>cluster manager</emphasis>.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Driver program</emphasis>, which is the program that directs the cluster manager and tells it what to do. In Apache Spark for Hadoop you have two choices: To run the Driver program as an external process ('yarn-client') or to run the Driver program as a process inside YARN itself ('yarn-cluster').
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Executor</emphasis>, which is a node in a Spark cluster that executes a partition (chunk) of a job.
				</para>
			</listitem>
		</orderedlist>
		<para>In the top-part of the below image you see Hadoop/YARN as well as Apache Spark, and how they are componentized.</para>
		<inlinemediaobject>
			<imageobject>
				<imagedata fileref="hadoop_deployment_overview.png" format="PNG" />
			</imageobject>
		</inlinemediaobject>
		<para>In the lower part of the image you see DataCleaner's directory structure on HDFS. As you can see, the usual configuration and job files are used, but placed on HDFS. A special JAR file is placed on HDFS to act as executable for the Apache Spark executors.</para>
	</section>

	<section id="hadoop_spark_setup">
		<title>Setting up Spark and DataCleaner environment</title>
		<para>
			In order to work, Apache Spark requires either of environmental variables
			<emphasis>HADOOP_CONF_DIR</emphasis>
			or
			<emphasis>YARN_CONF_DIR</emphasis>
			to a directory containing your Hadoop/Yarn configuration files such as
			<emphasis>core-site.xml</emphasis>
			,
			<emphasis>yarn-site.xml</emphasis>
			etc.
		</para>
		
		<section>
			<title>Upload configuration file to HDFS</title>
			
			<para>
				DataCleaner on Hadoop needs a regular
				<link linkend="chapter_configuration_file">DataCleaner configuration file</link> (conf.xml).
				It's best to upload this to the hadoop distributed file system (HDFS).
				We recommend putting this file into the path
				<emphasis>/datacleaner/conf.xml</emphasis>
				.
	
				Simple example of a configuration file (conf.xml) with a CSV datastore based on a HDFS file or directory:
			</para>
			
			<programlisting>
				&lt;?xml version="1.0" encoding="UTF-8"?&gt;
				&lt;configuration xmlns="http://eobjects.org/analyzerbeans/configuration/1.0"&gt;
				&#160;&lt;datastore-catalog&gt;
				&#160;&#160;&lt;csv-datastore name="mydata"&gt;
				&#160;&#160;&#160;&lt;filename&gt;hdfs://bigdatavm:9000/path/to/data.txt&lt;/filename&gt;
				&#160;&#160;&#160;&lt;multiline-values&gt;false&lt;/multiline-values&gt;
				&#160;&#160;&lt;/csv-datastore&gt;
				&#160;&lt;/datastore-catalog&gt;
				&lt;/configuration&gt;
			</programlisting>
			
			<para>
				Notice the filename which is here specified with scheme, hostname and port:
			</para>
			
			<programlisting>
				&lt;filename&gt;hdfs://bigdatavm:9000/path/to/data.txt&lt;/filename&gt;
			</programlisting>
			
			<para>
				This here refers to the Hadoop Namenode's hostname and port.
			</para>
			<para>
				It can also be specified more implicityly, without the username and port:
			</para>
			
			<programlisting>
				&lt;filename&gt;hdfs:///path/to/data.txt&lt;/filename&gt;
			</programlisting>
			
			<para>
				Or even without scheme:
			</para>
			
			<programlisting>
				&lt;filename&gt;/path/to/data.txt&lt;/filename&gt;
			</programlisting>
		</section>

		<section>
			<title>Upload job file to HDFS</title>
			
			<para>
				Upload the DataCleaner job you wish to run (a DataCleaner .analysis.xml job file) to HDFS.
				We recommend putting this file into a path such as <emphasis>/datacleaner/jobs/myjob.xml</emphasis>.
				The jobs can be built using the DataCleaner desktop UI, but do ensure that they map well to the configuration file also on HDFS.
			</para>
			<para>
				Example job file based on the above datastore:
			</para>
			<programlisting>
				&lt;?xml version="1.0" encoding="UTF-8"?&gt;
				&lt;job xmlns="http://eobjects.org/analyzerbeans/job/1.0"&gt;
				&#160;&lt;source&gt;
				&#160;&#160;&lt;data-context ref="mydata" /&gt;
				&#160;&#160;&lt;columns&gt;
				&#160;&#160;&#160;&lt;column id="col_country" path="country" /&gt;
				&#160;&#160;&#160;&lt;column id="col_company" path="company" /&gt;
				&#160;&#160;&lt;/columns&gt;
				&#160;&lt;/source&gt;
				&#160;&lt;analysis&gt;
				&#160;&#160;&lt;analyzer&gt;
				&#160;&#160;&#160;&lt;descriptor ref="Create CSV file"/&gt;
				&#160;&#160;&#160;&lt;properties&gt;
				&#160;&#160;&#160;&#160;&lt;property name="File" value="hdfs:///path/to/output.csv"/&gt;
				&#160;&#160;&#160;&#160;&lt;property name="Separator char" value="&amp;#44;"/&gt;
				&#160;&#160;&#160;&#160;&lt;property name="Quote char" value="&amp;quot;"/&gt;
				&#160;&#160;&#160;&#160;&lt;property name="Escape char" value="\"/&gt;
				&#160;&#160;&#160;&#160;&lt;property name="Include header" value="true"/&gt;
				&#160;&#160;&#160;&#160;&lt;property name="Encoding" value="UTF-8"/&gt;
				&#160;&#160;&#160;&#160;&lt;property name="Fields" value="[COUNTRY,CUSTOMERNUMBER]"/&gt;
				&#160;&#160;&#160;&#160;&lt;property name="Overwrite file if exists" value="true"/&gt;
				&#160;&#160;&#160;&lt;/properties&gt;
				&#160;&#160;&#160;&lt;input ref="col_country" name="Columns"/&gt;
				&#160;&#160;&#160;&lt;input ref="col_company" name="Columns"/&gt;
				&#160;&#160;&#160;&lt;/analyzer&gt;
				&#160;&lt;/analysis&gt;
				&lt;/job&gt;
			</programlisting>
			
			<para>This particular job is very simplistic - it just copies data from A to B. Notes about the job file contents:</para>
			
			<orderedlist>
				<listitem>
					<para>
						The job is referring to <emphasis>mydata</emphasis> which was the name of the CSV datastore
						defined in the configuration file.
					</para>
				</listitem>
				<listitem>
					<para>
						There is another HDFS file reference used in the "File" property.
						The filename format is the same as in the configuration file.
					</para>
				</listitem>
			</orderedlist>
			
			<para>If your desktop application has access to the namenode then you can build this job in the desktop application, save it and run it on spark. There is nothing particular about this job that makes it runnable on spark, except that the file references involved are resolvable from the hadoop nodes.</para>
		</section>
		
		<section>
			<title>Upload executables to HDFS</title>
			
			<para>In the installation of DataCleaner you will find the file 'DataCleaner-env-spark-[version].jar'.</para>

			<para>This jar file contains the core of what is needed to run DataCleaner with Apache Spark on Hadoop. It contains also the standard components of DataCleaner.</para>

			<para>Upload this jar file to HDFS in the folder <emphasis>/datacleaner/lib</emphasis>.</para>
			<para>Upload any extension jar files that you need (for instance Groovy-DataCleaner.jar) to that same folder.</para>

		</section>
	</section>

	<section id="hadoop_launch">
		<title>Launching DataCleaner jobs using Spark</title>

		<para>Go to the Spark installation path to run the job. Use the following command line template:</para>

		<programlisting>
			bin/spark-submit --class org.datacleaner.spark.Main --master yarn-cluster /path/to/DataCleaner-env-spark-x.x.x.jar
			/path/to/conf.xml /path/to/job_file.analysis.xml ([/path/to/custom_properties.properties])
		</programlisting>
		
		<para>
			A convenient way to organize it is in a shell script like the below, where every individual argument can be edited line by line:
		</para>
		
		<programlisting>
			#!/bin/sh
			SPARK_HOME=/path/to/apache-spark
			SPARK_MASTER=yarn-cluster
			DC_PRIMARY_JAR=/path/to/DataCleaner-env-spark.jar
			DC_EXTENSION_JARS=/path/to/extension1.jar,/path/to/extension2.jar
			DC_CONF_FILE=hdfs:///path/to/conf.xml
			DC_JOB_FILE=hdfs:///path/to/job_file.analysis.xml
			DC_PROPS=hdfs:///path/to/custom_properties.properties
			
			DC_COMMAND="$SPARK_HOME/bin/spark-submit"
			DC_COMMAND="$DC_COMMAND --class org.datacleaner.spark.Main"
			DC_COMMAND="$DC_COMMAND --master $SPARK_MASTER"
			
			echo "Using DataCleaner executable: $DC_PRIMARY_JAR"
			if [ "$DC_EXTENSION_JARS" != "" ]; then
			&#160;&#160;echo "Adding extensions: $DC_EXTENSION_JARS"
			&#160;&#160;DC_COMMAND="$DC_COMMAND --jars $DC_EXTENSION_JARS"
			fi

			DC_COMMAND="$DC_COMMAND $DC_PRIMARY_JAR $DC_CONF_FILE $DC_JOB_FILE $DC_PROPS"
			
			echo "Submitting DataCleaner job $DC_JOB_FILE to Spark $SPARK_MASTER"
			$DC_COMMAND
		</programlisting>

		<para>The example makes it clear that there are a few more parameters to invoking the job. Let's go through them:</para>
		
		<orderedlist>
			<listitem>
				<para>
					<emphasis>SPARK_MASTER</emphasis> represents the location of the Driver program, see the section on <link linkend="hadoop_deployment">Hadoop deployment overview</link>.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>DC_EXTENSION_JARS</emphasis> allows you to add additional JAR files with extensions to DataCleaner.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>DC_PROPS</emphasis> is maybe the most important one. It allows you to add a .properties file which can be used for a number of things:
				</para>
				<orderedlist>
					<listitem>
						<para>
							Special property
							<emphasis>datacleaner.result.hdfs.path</emphasis>
							which allows you to specify the filename (on HDFS) where the analysis result (.analysis.result.dat) file is stored. It defaults to
							<emphasis>/datacleaner/results/[job name]-[timestamp].analysis.result.dat</emphasis>
						</para>
					</listitem>
					<listitem>
						<para>
							Special property
							<emphasis>datacleaner.result.hdfs.enabled</emphasis>
							which can be either 'true' (default) or 'false'. Setting this property to false will disable result gathering completely from the
							DataCleaner job, which gives a significant increase in performance, but no analyzer results are gathered or written. This is thus only
							relevant for ETL-style jobs where the purpose of the job is to create/insert/update/delete from other datastores or files.
						</para>
					</listitem>
					<listitem>
						<para>Properties to <link linkend="cli_overriding_configuration_elements">override configuration defaults</link>.</para>
					</listitem>
					<listitem>
						<para>Properties to <link linkend="section_parameterizable_jobs">set job variables/parameters</link>.</para>
					</listitem>
				</orderedlist>
			</listitem>
		</orderedlist>
	</section>
	
	<section id="hadoop_limitations">
		<title>Limitations of the Hadoop interface</title>

		<para>While the Hadoop interface for DataCleaner allows distributed running of DataCleaner jobs on the Hadoop platform, there are a few limitations:</para>
	
		<orderedlist>
			<listitem>
				<para><emphasis>Datastore support</emphasis></para>
				<para>Currently we support a limited set of source datastores from HDFS. CSV files are the primary source here.
				We do require that files on HDFS are UTF8 encoded and that only single-line values occur.</para>
			</listitem>
			<listitem>
				<para><emphasis>Non-distributable components</emphasis></para>
				<para>A few components are by nature not distributable. If your job depends on these, DataCleaner will resort to
				executing the job on a single Spark executor, which may have a significant performance impact.</para>
			</listitem>
			<listitem>
				<para><emphasis>Hadoop Distributions without Namenode</emphasis></para>
				<para>Some Hadoop Distributions (such as MapR) have replaced the concept of Namenode with something else.
				This is mostly fine, but it does mean that file paths with username+port of Namenodes are obviously not working.</para>
			</listitem>
		</orderedlist>
	</section>

</chapter>
